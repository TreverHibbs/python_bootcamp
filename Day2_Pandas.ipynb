{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Python Bootcamp Day2\n",
    "This notebook contains explenation and examples of python code with a focus on the pandas library and python iteration.\n",
    "\n",
    "## Getting Started \n",
    "Go to kaggle.com and create an account. ->\n",
    "https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F\n",
    "\n",
    "Once you have created an account and have signed in you must locate and open the provided bootcamp notebook.<br>\n",
    "1. navigate to the *notebook* tab \n",
    "2. Search public notebooks for \"Day2_Pandas\"\n",
    "3. Click on the notebook titled \"Day2_Pandas\" by ACMW WSUV\n",
    "4. At the top right corner of the screen click on the elipsi and from the drop down menu select copy and edit kernel\n",
    "\n",
    "Now you must add the necisary data to the kaggle notebook environment\n",
    "1. click on \"Add Data\"\n",
    "2. Search for Russian troll tweets\n",
    "3. select the data from user FiveThirtyEight titled Russian Troll Tweets\n",
    "4. click add\n",
    "5. click yes <br>\n",
    "\n",
    "Your environement is now ready to go! ðŸ˜˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example of a csv file opened in a text editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://imgur.com/sFOPDPM\"><img src=\"https://i.imgur.com/sFOPDPM.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Initialize pandas DataFrames with csv files input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t1 = pd.read_csv(\"IRAhandle_tweets_1.csv\")\n",
    "#t2 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_2.csv\")\n",
    "#t3 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_3.csv\")\n",
    "#t4 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_4.csv\")\n",
    "#t5 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_5.csv\")\n",
    "#t6 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_6.csv\")\n",
    "#t7 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_7.csv\")\n",
    "#t8 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_8.csv\")\n",
    "#t9 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_9.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want to put all the dataframes together into one large data frame. If I use .merge() I would then\n",
    "need to keep track of inner joins vs outer join and duplicates can cause unexpected results. So I will us .concat().\n",
    "\n",
    "The concat() function (in the main pandas namespace) does all of the heavy lifting of performing concatenation operations \n",
    "along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes. \n",
    "Note that I say â€œif anyâ€ because there is only a single possible axis of concatenation for Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frames = [t1,t2,t3,t4,t5,t6,t7,t8,t9]\n",
    "tweets = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We can print out the titles of all the columns, this can help us organize our information\n",
    "print(list(t1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can display in a terminal using print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or we can display them in a jupyter format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " We are going to narrow the information to the year 2016, which was an election year and then display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['publish_date'].str.contains(\"2016\")] \n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to narrow the information further to only the tweets that are in English and display again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['language'].str.contains(\"English\")] \n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to narrow the information further to only the tweets that contain the word Trump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['content'].str.contains(\"Trump\")] \n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pandas DataFrames lets you drop or select specific columns\n",
    "\n",
    "drop by Name of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['external_author_id', 'post_type','updates','new_june_2018','retweet','harvested_date'], axis=1)\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also select specific columns. For this excercise we only need the content column.\n",
    "\n",
    "Now we have all English language tweets from known Russian twitter accounts, posted in 2016, which was an election year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweets = tweets[['content']]\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this point we can do a lot with the information. Such as finding the top 20 most tweeted words based off of our previous filters. \n",
    "  \n",
    "To do so we need to iterate through each row and split it into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#iloc is panda specific\n",
    "rows = tweets.iloc[0]['content'].split()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Using split, we need to create a list of words\n",
    "* With each row we split, iterate through the list adding every unique word to a dictionary\n",
    "\n",
    "* If we haven't seen the word, add the word and start the count\n",
    "* If we have seen the word, update its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Declare a dictionary\n",
    "mainDict= {}\n",
    "\n",
    "for index, row in tweets.iterrows(): \n",
    "    rows = row['content'].split()\n",
    "    \n",
    "    for w in rows:\n",
    "        if w in mainDict:\n",
    "            mainDict[w] = mainDict[w] + rows.count(w)\n",
    "        else:\n",
    "            mainDict[w] = rows.count(w)\n",
    "\n",
    "print (mainDict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can remove some of the words such as prepositions, conjunctions, articles, auxillary verbs, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(len(mainDict))\n",
    "\n",
    "wordDel = ['in','you','the','The','I','is','on','a','A','to','To','of','for','and','it']\n",
    "for w in wordDel:\n",
    "    mainDict.pop(w)\n",
    "\n",
    "wordDel2 = ['will','with','that','this','be','at','as','he','his','by','not','they']\n",
    "for x in wordDel2:\n",
    "    mainDict.pop(x)\n",
    "\n",
    "wordDel3 = ['-','&','about','has','from','was','have','who','all','say','my','out']\n",
    "for z in wordDel3:\n",
    "    mainDict.pop(z)\n",
    "\n",
    "wordDel4 = ['says','up','but','we','like','are','if','our','via','ï¿½','This','your','an']\n",
    "for y in wordDel4:\n",
    "    mainDict.pop(y)\n",
    "    \n",
    "print (len(mainDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We can sort the dictionary in descending order\n",
    "mainDict = {k: v for k, v in sorted(mainDict.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "counter=0\n",
    "for k, v in mainDict.items():\n",
    "    if counter > 19:\n",
    "        break\n",
    "    print(v, \"\\t\", k)\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# You can also turn the mainDict into a list with just the keys and print the top 20\n",
    "topTwenty = list(mainDict.keys())\n",
    "print(topTwenty[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "IRAhandle_tweets_1 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_1.csv\")\n",
    "IRAhandle_tweets_2 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_2.csv\")\n",
    "IRAhandle_tweets_3 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_3.csv\")\n",
    "IRAhandle_tweets_4 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_4.csv\")\n",
    "IRAhandle_tweets_5 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_5.csv\")\n",
    "IRAhandle_tweets_6 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_6.csv\")\n",
    "IRAhandle_tweets_7 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_7.csv\")\n",
    "IRAhandle_tweets_8 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_8.csv\")\n",
    "IRAhandle_tweets_9 = pd.read_csv(\"../input/russian-troll-tweets/IRAhandle_tweets_9.csv\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
